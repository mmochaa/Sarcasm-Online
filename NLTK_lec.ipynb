{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "DzWJnGxrX43R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing NLTK using your computer terminal\n",
        "\n",
        "pip is the Python command to install packages"
      ],
      "metadata": {
        "id": "VKrq6JuZUasv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMXSLyKEUGLm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the package"
      ],
      "metadata": {
        "id": "1VQ8pZ_JVA1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Pm-YNtqZVDg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating data"
      ],
      "metadata": {
        "id": "oxbE7ApbUx1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download a specific corpus (e.g., 'punkt' for tokenization, splits text into sentences)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download a different corpus (e.g., 'averaged_perceptron_tagger' for part-of-speech tagging)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Download all NLTK corpora (can take a significant amount of time and disk space)\n",
        "# nltk.download('all')\n",
        "\n",
        "print(\"NLTK corpora downloaded.\")\n"
      ],
      "metadata": {
        "id": "K37AFBnCU0wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You can also generate your own data"
      ],
      "metadata": {
        "id": "dVFSjXmEWhQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"In 2022, Dr. Alice Morgan moved to New York City to join Columbia University as a professor of computational linguistics.\",\n",
        "    \"The Amazon rainforest is home to millions of species and is often called the lungs of the Earth.\",\n",
        "    \"On July 4th, the fireworks in Washington, D.C. lit up the sky as thousands gathered to celebrate Independence Day.\",\n",
        "    \"Tesla announced its new electric truck model in California, focusing on sustainability and autonomous driving.\",\n",
        "    \"Researchers at MIT developed a novel algorithm that improves machine translation accuracy in low-resource languages.\",\n",
        "    \"The Grand Canyon offers breathtaking views and attracts millions of visitors each year for hiking, rafting, and photography.\",\n",
        "    \"In a surprising turn of events, the local bakery in Seattle won the national Best Croissant competition in 2023.\",\n",
        "    \"Mount Everest, the tallest mountain in the world, challenges even the most experienced climbers with its altitude and severe weather conditions.\",\n",
        "    \"Shakespeareâ€™s plays continue to influence literature and drama courses around the globe, centuries after their creation.\",\n",
        "    \"The Nobel Peace Prize was awarded to a climate activist who spearheaded global initiatives on carbon reduction.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "UxuwfVQHWkmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "XHVY4UqMYEou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization"
      ],
      "metadata": {
        "id": "jgoDRJY4cB81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing\n",
        "lower_corpus = [text.lower() for text in corpus]\n",
        "print(\"Lowercased Corpus:\")\n",
        "print(lower_corpus)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Removing punctuation\n",
        "import string\n",
        "tokenized_no_punct_corpus = [[word for word in tokens if word not in string.punctuation] for tokens in tokenized_corpus]\n",
        "print(\"Tokenized Corpus (no punctuation):\")\n",
        "print(tokenized_no_punct_corpus)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Removing numbers\n",
        "tokenized_no_nums_corpus = [[word for word in tokens if not word.isdigit()] for tokens in tokenized_no_punct_corpus]\n",
        "print(\"Tokenized Corpus (no numbers):\")\n",
        "print(tokenized_no_nums_corpus)\n",
        "print(\"-\" * 30)\n",
        "\n"
      ],
      "metadata": {
        "id": "kQuLAlVbYJm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "yhUa3OH4cD-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Sentence Tokenization\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_tokens = [sent_tokenize(text) for text in corpus]\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentence_tokens)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# #### Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens = [word_tokenize(text) for text in corpus]\n",
        "print(\"Word Tokenization:\")\n",
        "print(word_tokens)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# You can also store these tokenized lists for further processing\n",
        "tokenized_corpus = word_tokens"
      ],
      "metadata": {
        "id": "7b8V2CeBYOcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove stop words"
      ],
      "metadata": {
        "id": "-PXpuOYkccoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokenized corpus\n",
        "tokenized_no_stopwords_corpus = [[word for word in tokens if word not in stop_words] for tokens in tokenized_no_nums_corpus]\n",
        "\n",
        "print(\"Tokenized Corpus (no stop words):\")\n",
        "print(tokenized_no_stopwords_corpus)\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "BUpf-ZL6cIGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "A4WLmTqHctRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the WordNet corpus\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in tokenized_no_stopwords_corpus]\n",
        "print(\"Lemmatized Corpus:\")\n",
        "print(lemmatized_corpus)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example with POS tag consideration for better lemmatization\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "  \"\"\"Maps POS tags to WordNet tags.\"\"\"\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_corpus_pos = [[lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens] for tokens in tokenized_no_stopwords_corpus]\n",
        "print(\"Lemmatized Corpus (with POS):\")\n",
        "print(lemmatized_corpus_pos)\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "kuc2DT5wcj9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "TqjS8EqCcvrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_corpus = [[stemmer.stem(word) for word in tokens] for tokens in tokenized_no_stopwords_corpus]\n",
        "print(\"Stemmed Corpus:\")\n",
        "print(stemmed_corpus)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "example_words = [\"running\", \"jumps\", \"played\", \"flies\", \"better\", \"beautifully\"]\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"More Stemming Examples:\")\n",
        "for word in example_words:\n",
        "  stemmed_word = stemmer.stem(word)\n",
        "  print(f\"'{word}' stemmed to '{stemmed_word}'\")\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "kjauZrI2cwsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency parsing\n",
        "\n",
        "\n",
        "Dependency parsing: Understanding grammatical relationships between words in a sentence.\n",
        "It reveals how words modify or relate to each other, forming the sentence structure."
      ],
      "metadata": {
        "id": "KFPp5fv9dEON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required parser\n",
        "nltk.download('dependency_treebank') # Download a dependency parsed corpus for potential future use\n",
        "nltk.download('universal_tagset') # For Universal POS tags\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Perform POS tagging (needed for dependency parsing)\n",
        "tagged_sentence = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "\n",
        "# NLTK doesn't have a built-in high-performance dependency parser.\n",
        "# However, we can illustrate the concept and use a simple parser if needed\n",
        "# or demonstrate the structure with a library like SpaCy or StanfordCoreNLP.\n",
        "\n",
        "# For NLTK, you would typically train a parser or use a pre-trained one if available.\n",
        "# As a demonstration of the concept and output format:\n",
        "\n",
        "# To use a statistical dependency parser in NLTK, you'd need a trained model.\n",
        "# A common approach is to train one on a treebank annotated with dependencies.\n",
        "# For simplicity and illustration, let's show the desired output structure\n",
        "# which represents the head-modifier relationships.\n",
        "\n",
        "# Let's use a simple example to illustrate the concept of head-modifier pairs.\n",
        "# This is a manual representation, not generated by an NLTK parser out-of-the-box.\n",
        "\n",
        "dependency_structure = [\n",
        "    (\"fox\", \"The\", \"det\"),\n",
        "    (\"fox\", \"quick\", \"amod\"),\n",
        "    (\"fox\", \"brown\", \"amod\"),\n",
        "    (\"jumps\", \"fox\", \"nsubj\"),\n",
        "    (\"jumps\", \"over\", \"prep\"),\n",
        "    (\"over\", \"dog\", \"pobj\"),\n",
        "    (\"dog\", \"the\", \"det\"),\n",
        "    (\"dog\", \"lazy\", \"amod\")\n",
        "]\n",
        "\n",
        "print(\"\\nExample sentence:\")\n",
        "print(sentence)\n",
        "\n",
        "print(\"\\nExample Dependency Relationships (Manual Illustration):\")\n",
        "for head, modifier, relation in dependency_structure:\n",
        "  print(f\"  - '{modifier}' is related to '{head}' with relationship '{relation}'\")\n",
        "\n",
        "# To get actual dependency trees, you would typically use libraries like SpaCy\n",
        "# or integrate with external parsers like Stanford CoreNLP.\n",
        "# If you were to use a NLTK trained parser (which is not straightforward without training):\n",
        "# parser = <trained_nltk_parser>\n",
        "# tree = parser.parse(tagged_sentence)\n",
        "# print(\"\\nExample Dependency Tree (Conceptual, requires trained parser):\")\n",
        "# tree.pretty_print()\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "r1iEjz8Vc3ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Spacy"
      ],
      "metadata": {
        "id": "evQyn2mUeATf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(\"\\nExample Dependency Parsing with spaCy:\")\n",
        "# Iterate over the tokens and print their dependency relations\n",
        "for token in doc:\n",
        "  print(f\"  Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Tag: {token.tag_}, Dep: {token.dep_}, Head: {token.head.text}, Head_POS: {token.head.pos_}\")\n",
        "\n",
        "# Visualize the dependency tree (requires installing displacy if you want a graphical representation)\n",
        "# from spacy import displacy\n",
        "# displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "aAO1UqZ6d31t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER"
      ],
      "metadata": {
        "id": "b8RzSRbNeFdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the necessary resource for NE chunking\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Example sentence for NER\n",
        "ner_sentence = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(ner_sentence)\n",
        "\n",
        "# Part-of-speech tag the tokens\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform Named Entity Recognition\n",
        "ne_tree = nltk.ne_chunk(tagged)\n",
        "\n",
        "print(\"Named Entity Recognition Example:\")\n",
        "print(ne_tree)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# You can also iterate through the chunks to see recognized entities\n",
        "print(\"Recognized Entities:\")\n",
        "for chunk in ne_tree:\n",
        "  if hasattr(chunk, 'label'):\n",
        "    print(f\"  Entity: {' '.join(c[0] for c in chunk)}, Type: {chunk.label()}\")\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "IQMn9T56eGvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}